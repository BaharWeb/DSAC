@inproceedings{Bakaev2018APEIE,
author = {Bakaev, Maxim and Laricheva, Tatiana A and Heil, Sebastian and Gaedke, Martin},
booktitle = {2018 XIV International Scientific-Technical Conference on Actual Problems of Electronics Instrument Engineering (APEIE)},
doi = {10.1109/APEIE.2018.8545807},
file = {:Users/sebastian/Dropbox/PhD Materials/MyPublications/Visual/08545807.pdf:pdf},
isbn = {978-1-5386-7054-5},
keywords = {complexity,meAuthor},
mendeley-tags = {complexity,meAuthor},
month = {oct},
pages = {381--385},
publisher = {IEEE},
title = {{Analysis and Prediction of University Websites Perceptions by Different User Groups}},
url = {https://ieeexplore.ieee.org/document/8545807/},
year = {2018}
}
@article{Bakaev2019JWE,
author = {Bakaev, Maxim and Heil, Sebastian and Khvorostov, Vladimir and Gaedke, Martin},
doi = {10.13052/jwe1540-9589.17676},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Bakaev et al. - 2019 - Auto-Extraction and Integration ofMetrics for Web User Interfaces.pdf:pdf},
issn = {1540-9589},
journal = {Journal of Web Engineering},
keywords = {meAuthor},
mendeley-tags = {meAuthor},
number = {6},
pages = {561--590},
title = {{Auto-Extraction and Integration of Metrics for Web User Interfaces}},
url = {http://www.riverpublishers.com/journal{\_}read{\_}html{\_}article.php?j=JWE/17/6/6},
volume = {17},
year = {2019}
}
@inproceedings{Bakaev2017Kansei,
abstract = {{\textcopyright} 2017 IEEE. Ensuring similarity of user interfaces (UI) is often desirable, e.g. in software migration and redesign projects, to minimize experience disruption for regular users and increase subjective satisfaction with new versions. In our paper we explore applicability of artificial neural networks (ANNs) to support test-driven development by predicting similarity assessments without employing the actual users. Having reviewed requirements engineering (RE) standards and practices for HCI-related requirements, we identified two dimensions for similarity of web UIs: 1) objective, the data for which we collected with a dedicated web intelligence miner and 2) user-subjective, operationalized with the renowned Kansei Engineering method. Then we constructed the respective ANN models predicting perceived similarity between websites of a same domain and trained the models with the data we collected in experimental sessions with 209 participants of different nationalities and 21 operational university websites. The results of our pilot study suggest that subjective "emotional" factors are considerably more important in predicting similarity evaluations provided by users. Thus, employment of trained ANNs as test oracles may be feasible in automated measurement and control of UI similarity.},
author = {Bakaev, Maxim and Khvorostov, Vladimir and Heil, Sebastian and Gaedke, Martin},
booktitle = {2017 IEEE 25th International Requirements Engineering Conference Workshops (REW)},
doi = {10.1109/REW.2017.13},
file = {:Users/sebastian/Dropbox/PhD Materials/MyPublications/Visual/Evaluation of Web UI Similarity with Kansai Engineering.pdf:pdf},
isbn = {978-1-5386-3488-2},
keywords = {Kansei engineering,Neural networks,Non-functional requirements,Re-use,Software quality,Usability evaluation,meAuthor,similarity,subjective similarity},
mendeley-tags = {meAuthor,similarity,subjective similarity},
month = {sep},
pages = {125--131},
publisher = {IEEE},
title = {{Evaluation of User-Subjective Web Interface Similarity with Kansei Engineering-Based ANN}},
url = {http://ieeexplore.ieee.org/document/8054840/},
year = {2017}
}
@incollection{Bakaev2018ICWE,
abstract = {{\textcopyright} Springer International Publishing AG, part of Springer Nature 2018. Most techniques for webpage structure and design mining are based on code analysis and are detached from a human user's perception of the web user interface (WUI). Our paper is dedicated to approaches that instead focus on analysis of webpage's visual representation – the way it is rendered in different browsers and environments and delivered to the end user. Specifically, we describe the software tool that we built, which takes a WUI screenshot and produces structured and machine-readable representation (JSON) of interface elements as made out by a human user. The implementation is based on OpenCV (image recognition functions), dlib (trained detector for the elements' classification), and Tesseract (label and content text recognition). To demonstrate feasibility of the approach, we describe application of our analyzer tool to auto-calculate certain measures for a WUI and to predict users' subjective impressions. Particularly, we assess UI visual complexity, which is known to significantly influence both cognitive and affective aspects of interaction. The results suggest the analyzer's output is mostly characteristic of the users' visual perception and can be useful for auto-assessing and comparing WUIs.},
author = {Bakaev, Maxim and Heil, Sebastian and Khvorostov, Vladimir and Gaedke, Martin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-91662-0_10},
file = {:Users/sebastian/Downloads/AS6362392921661511528702676236{\_}content{\_}1.pdf:pdf},
isbn = {9783319916613},
issn = {16113349},
keywords = {HCI vision,Human factors,Image recognition,Visual complexity,Web design mining,meAuthor},
mendeley-tags = {meAuthor},
pages = {136--144},
title = {{HCI Vision for Automated Analysis and Mining of Web User Interfaces}},
url = {http://link.springer.com/10.1007/978-3-319-91662-0{\_}10},
volume = {10845 LNCS},
year = {2018}
}
@inproceedings{Bakaev2019ICWE,
abstract = {We present a software tool for collecting web UI metrics from differ- ent providers and integrating them in a single database for further analysis. The platform's architecture supports both code- and image-based UI assessment, thus allowing to combine advantages of the two approaches. The data structures are based on a web UI measurement domain ontology (OWL) that organizes the currently disperse set of metrics and services. Our platform can be of use to in- terface designers, researchers, and UI analysis tools developers. Keywords:},
address = {Daejeon, Korea},
author = {Bakaev, Maxim and Heil, Sebastian and Perminov, Nikita and Gaedke, Martin},
booktitle = {To appear in: Proceedings of 19th International Conference on Web Engineering},
file = {:Users/sebastian/Downloads/Bakaev{\_}ICWE{\_}Poster final.pdf:pdf},
keywords = {meAuthor},
mendeley-tags = {meAuthor},
publisher = {Springer},
title = {{Integration Platform for Metric-Based Analysis of Web User Interfaces}},
year = {2019}
}
@inproceedings{Heil2016Similarity,
abstract = {Given the rapid update cycles in modern web information systems and the abundance of legacy software being migrated to the web,controlling similarity between user interfaces (UI) is an actual problem of interaction engineering. The similarity (consistency) aspect is also increasingly considered in computer-aided design,where it is included in the optimized goal function,to minimize re-learning effort for users. In this paper,we explore the impact of the proposed layout distance measure,which is calculated for different levels of hierarchy in web UIs,which we identify as: Region – Block – Group – Element. To support our approach,we conducted an experimental pilot study in the context of an ongoing medical information system (IS) web migration project. The regression analysis suggests that layout distance (particularly,its orientation dimension) does have effect on web UI similarity as perceived by users. The results can be used by web engineers,in particular to smoothen the transition between versions of a UI for users and IS operators.},
address = {Cham},
author = {Heil, Sebastian and Bakaev, Maxim and Gaedke, Martin},
booktitle = {Web Information Systems Engineering -- WISE 2016},
doi = {10.1007/978-3-319-48740-3_18},
editor = {Cellary, Wojciech and Mokbel, Mohamed F. and Wang, Jianmin and Wang, Hua and Zhou, Rui and Zhang, Yanchun},
file = {:Users/sebastian/Dropbox/PhD Materials/MyPublications/Visual/2016-PAP-WISE2016-UIX Migration/Camera Ready/20160822-PUB-WISE{\_}Paper{\_}short.pdf:pdf},
isbn = {9783319487397},
keywords = {Similarity measure,User interface,Web migr,[HCI,meFirstAuthor},
mendeley-tags = {meFirstAuthor},
publisher = {Springer International Publishing},
title = {{Measuring and Ensuring Similarity of User Interfaces: The Impact of Web Layout}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-48740-3{\_}18},
volume = {10041 LNCS},
year = {2016}
}
@incollection{Bakaev2017WebIntelligence,
abstract = {{\textcopyright} Springer International Publishing AG 2017. Code and design reuse are as old as software engineering industry itself, but it's also always a new trend, as more and more software products and websites are being created. Domain-specific design reuse on the web has especially high potential, saving work effort for thousands of developers and encouraging better interaction quality for millions of Internet users. In our paper we perform pilot feature engineering for finding similar solutions (website designs) within Domain, Task, and User UI models supplemented by Quality aspects. To obtain the feature values, we propose extraction of website-relevant data from online global services (DMOZ, Alexa, SimilarWeb, etc.) considered as linked open data sources, using specially developed web intelligence data miner. The preliminary investigation with 21 websites and 82 human annotators showed reasonable accuracy of the data sources and suggests potential feasibility of the approach.},
author = {Bakaev, Maxim and Khvorostov, Vladimir and Heil, Sebastian and Gaedke, Martin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-60131-1_22},
file = {:Users/sebastian/Downloads/ICWE - Bakaev - final.pdf:pdf},
isbn = {9783319601304},
issn = {16113349},
keywords = {Data mining,Linked data quality,Model-driven development,Software reuse,Web design patterns,meAuthor},
mendeley-tags = {meAuthor},
pages = {370--377},
title = {{Web Intelligence Linked Open Data for Website Design Reuse}},
url = {http://link.springer.com/10.1007/978-3-319-60131-1{\_}22},
volume = {10360 LNCS},
year = {2017}
}
@inproceedings{Khadka2014ProfessionalsModernization,
abstract = {Existing research in legacy system modernization has traditionally focused on technical challenges, and takes the standpoint that legacy systems are obsolete, yet crucial for an organization's operation. Nonetheless, it remains unclear whether practitioners in the industry also share this perception. This paper describes the outcome of an exploratory study in which 26 industrial practitioners were interviewed on what makes a software system a legacy system, what the main drivers are that lead to the modernization of such systems, and what challenges are faced during the modernization process. The findings of the interviews have been validated by means of a survey with 198 respondents. The results show that practitioners value their legacy systems highly, the challenges they face are not just technical, but also include business and organizational aspects.},
address = {New York, New York, USA},
author = {Khadka, Ravi and Batlajery, Belfrit V. and Saeidi, Amir M. and Jansen, Slinger and Hage, Jurriaan},
booktitle = {Proceedings of the 36th International Conference on Software Engineering - ICSE 2014},
doi = {10.1145/2568225.2568318},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Khadka et al. - 2014 - How do professionals perceive legacy systems and software modernization.pdf:pdf},
isbn = {9781450327565},
keywords = {grounded theory,legacy modernization,legacy systems},
pages = {36--47},
publisher = {ACM Press},
title = {{How do professionals perceive legacy systems and software modernization?}},
url = {http://dl.acm.org/citation.cfm?doid=2568225.2568318},
year = {2014}
}
@inproceedings{Jahanian2017,
address = {New York, New York, USA},
author = {Jahanian, Ali and Isola, Phillip and Wei, Donglai},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA '17},
doi = {10.1145/3027063.3053238},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Jahanian, Isola, Wei - 2017 - Mining Visual Evolution in 21 Years of Web Design.pdf:pdf},
isbn = {9781450346566},
pages = {2676--2682},
publisher = {ACM Press},
title = {{Mining Visual Evolution in 21 Years of Web Design}},
url = {http://dl.acm.org/citation.cfm?doid=3027063.3053238},
year = {2017}
}
@article{Chechik2009,
abstract = {Learning a measure of similarity between pairs of objects is a fundamental prob- lemin machine learning. It stands in the core of classification methods like kernel machines, and is particularly useful for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, current approaches for learning similarity do not scale to large datasets, especially when imposingmetric constraints on the learned similarity. We describe OASIS, a method for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features. Scalability is achieved through online learning of a bilinear model over sparse representations using a large margin criterion and an efficient hinge loss cost. OASIS is accurate at a wide range of scales: on a stan- dard benchmark with thousands of images, it is more precise than state-of-the-art methods, and faster by orders of magnitude. On 2.7 million images collected from the web, OASIS can be trained within 3 days on a single CPU. The non- metric similarities learned by OASIS can be transformed into metric similarities, achieving higher precisions than similarities that are learned as metrics in the first place. This suggests an approach for learning a metric from data that is larger by orders of magnitude than was handled before.},
author = {Chechik, Gal and Shalit, Uri},
file = {:Users/sebastian/Downloads/chechik{\_}nips2009.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{An Online Algorithm for Large Scale Image Similarity Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.8937{\&}rep=rep1{\&}type=pdf},
volume = {21},
year = {2009}
}
@article{Chechik2010,
author = {Chechik, Gal},
file = {:Users/sebastian/Downloads/chechik10a.pdf:pdf},
issn = {1532-4435},
pages = {1109--1135},
title = {{[JMLR2010]Large Scale Online Learning of Image Similarity Through Ranking.pdf}},
volume = {11},
year = {2010}
}
@inproceedings{Walsh2017ReDeCheck,
abstract = {Since people frequently access websites with a wide variety of de- vices (e.g., mobile phones, laptops, and desktops), developers need frameworks and tools for creating layouts that are useful at many viewport widths. While responsive web design (RWD) principles and frameworks facilitate the development of such sites, there is a lack of tools supporting the detection of failures in their layout. Since the quality assurance process for responsively designed web- sites is oſten manual, time-consuming, and error-prone, this paper presents ReDeCheck, an automated layout checking tool that alerts developers to both potential unintended regressions in responsive layout and common types of layout failure. In addition to sum- marizing ReDeCheck's benefits, this paper explores two different usage scenarios for this tool that is publicly available on GitHub.},
address = {New York, New York, USA},
author = {Walsh, Thomas A. and Kapfhammer, Gregory M. and McMinn, Phil},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis - ISSTA 2017},
doi = {10.1145/3092703.3098221},
file = {:Users/sebastian/Downloads/c49.pdf:pdf},
isbn = {9781450350761},
keywords = {2017,acm reference format,and phil mcminn,gregory m,kap ammer,layout failures,omas a,presentation failures,r,responsive web design,walsh},
pages = {360--363},
publisher = {ACM Press},
title = {{ReDeCheck: an automatic layout failure checking tool for responsively designed web pages}},
url = {http://dl.acm.org/citation.cfm?doid=3092703.3098221},
year = {2017}
}
@techreport{ISO/IEEE2017Measurement,
author = {ISO/IEEE},
booktitle = {ISO/IEC/IEEE 15939:2017},
file = {:Users/sebastian/Downloads/15939-2017.pdf:pdf},
keywords = {standard},
mendeley-tags = {standard},
title = {{ISO/IEC/IEEE International Standard - Systems and software engineering - Measurement process}},
year = {2017}
}
@book{RayLiu2002,
abstract = {Describing non-parametric and parametric theoretic classification and the training of discriminant functions, this second edition includes new and expanded sections on neural networks, Fisher's discriminant, wavelet transform, and the method of principal components. It contains discussions on dimensionality reduction and feature selection, novel computer system architectures, proven algorithms for solutions to common roadblocks in data processing, computing models including the Hamming net, the Kohonen self-organizing map, and the Hopfield net, detailed appendices with data sets illustrating key concepts in the text, and more.},
author = {Bow, Sing-Tze},
edition = {2},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Bow - 2002 - Pattern Recognition and Image Preprocessing(2).pdf:pdf},
isbn = {0824706595},
publisher = {CRC Press Inc},
title = {{Pattern Recognition and Image Preprocessing}},
url = {http://index-of.co.uk/Artificial-Intelligence/Pattern recognition and image preprocessing  2nd ed -Sing T. Bow.pdf},
year = {2002}
}
@article{Mckeeman1998DifferentialTesting,
author = {Mckeeman, William M and Problem, The Testing},
doi = {10.1.1.83.445},
file = {:Users/sebastian/Downloads/DifferentialTestingForSoftware.pdf:pdf},
journal = {DIGITAL TECHNICAL JOURNAL},
number = {1},
pages = {100----107},
title = {{DifferentialTestingForSoftware}},
volume = {10},
year = {1998}
}
@article{Lucia2008,
abstract = {This paper presents the research results of an ongoing technology transfer project carried out in coopera- tion between the University of Salerno and a small software company. The project is aimed at developing and transferring migration technology to the industrial partner. The partner should be enabled to migrate monolithic multi-user COBOL legacy systems to a multi-tier Web-based architecture. The assessment of the legacy systems of the partner company revealed that these systems had a very low level of decompos- ability with spaghetti-like code and embedded control flow and database accesses within the user interface descriptions. For this reason, it was decided to adopt an incremental migration strategy based on the reengineering of the user interface using Web technology, on the transformation of interactive legacy programs into batch programs, and the wrapping of the legacy programs. A middleware framework links the new Web-based user interface with the Wrapped Legacy System. An Eclipse plug-in, named MELIS (migration environment for legacy information systems), was also developed to support the migration process. Both the migration strategy and the tool have been applied to two essential subsystems of the most business critical legacy system of the partner company. Copyright {\textcopyright} 2008 John Wiley {\&} Sons, Ltd. Received},
annote = {From Duplicate 1 (Developing legacy system migration methods and tools for technology transfer - Lucia, Andrea De; Francese, Rita; Scanniello, Giuseppe; Tortora, Genoveffa; De Lucia, Andrea; Francese, Rita; Scanniello, Giuseppe; Tortora, Genoveffa)

From Duplicate 2 (Developing legacy system migration methods and tools for technology transfer - Lucia, Andrea De; Francese, Rita; Scanniello, Giuseppe; Tortora, Genoveffa)

incremental, wrapping (encapsulation) approach combined with UI reengineering

complete approach (all ReMiP disciplines)

SME

From Duplicate 2 (Developing legacy system migration methods and tools for technology transfer - Lucia, Andrea De; Francese, Rita; Scanniello, Giuseppe; Tortora, Genoveffa)

incremental, wrapping (encapsulation) approach combined with UI reengineering

complete approach (all ReMiP disciplines)},
archivePrefix = {arXiv},
arxivId = {1008.1900},
author = {Lucia, Andrea De and Francese, Rita and Scanniello, Giuseppe and Tortora, Genoveffa and {De Lucia}, Andrea and Francese, Rita and Scanniello, Giuseppe and Tortora, Genoveffa},
doi = {10.1002/spe.870},
eprint = {1008.1900},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Lucia et al. - 2008 - Developing legacy system migration methods and tools for technology transfer.pdf:pdf;:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/De Lucia et al. - 2008 - Developing legacy system migration methods and tools for technology transfer.pdf:pdf},
issn = {00380644},
journal = {Software: Practice and Experience},
keywords = {CYC Window Toolkit,Directx,Linux,MELIS,Mac OS x,OpenGL,SME,Windows,candidate,complete coverage},
mendeley-tags = {MELIS,SME,candidate,complete coverage},
month = {nov},
number = {13},
pages = {1333--1364},
title = {{Developing legacy system migration methods and tools for technology transfer}},
url = {http://doi.wiley.com/10.1002/spe.870},
volume = {38},
year = {2008}
}
@article{Oliva2001GIST,
abstract = {In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.},
author = {Oliva, Aude and Torralba, Antonio},
doi = {https://doi.org/10.1023/A:1011139631724},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Oliva, Torralba - 2014 - Modeling the Shape of the Scene A Holistic Representation of the Spatial Envelope.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {GIST,GIST features,energy spectrum,natural images,principal components,scene recognition,spatial layout},
mendeley-tags = {GIST,GIST features},
number = {3},
pages = {145--175},
publisher = {Kluwer Academic Publishers},
title = {{Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope}},
url = {https://link.springer.com/article/10.1023/A:1011139631724},
volume = {42},
year = {2001}
}
@article{Chechik2010OASIS,
author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
file = {:Users/sebastian/Downloads/chechik10a.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {OASIS},
mendeley-tags = {OASIS},
month = {mar},
number = {3/1/2010},
pages = {1109--1135},
publisher = {JMLR.org},
title = {{Large Scale Online Learning of Image Similarity Through Ranking}},
url = {http://dl.acm.org/citation.cfm?id=1756006.1756042},
volume = {11},
year = {2010}
}
@article{Chechik:2010:LSO:1756006.1756042,
author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {1109--1135},
publisher = {JMLR.org},
title = {{Large Scale Online Learning of Image Similarity Through Ranking}},
url = {http://dl.acm.org/citation.cfm?id=1756006.1756042},
volume = {11},
year = {2010}
}
@inproceedings{Distante2006a,
abstract = {This paper reports on a case study of redesigning a legacy application for the Web using the Ubiquitous Web Applications Design Framework with an extended version of its Transaction Design Model (UWAT+). Web application design methodologies hold the promise of engineering high-quality and long-lived Web systems and rich Internet applications. However, many such techniques focus solely on green-field development, and do not properly address the situation of leveraging the value locked in legacy systems. The redesign process supported by UWAT+ holistically blends design recovery technologies for capturing the know-how embedded in the legacy application with forward design methods particularly well suited for Web-based systems. The case study highlights some of the benefits of using UWAT+ in this context, as well as identifying possible areas for improvement in the redesign process and opportunities for tool automation to support it.},
author = {Distante, Damiano and Tilley, Scott and Canfora, Gerardo},
booktitle = {Conference on Software Maintenance and Reengineering (CSMR'06)},
doi = {10.1109/CSMR.2006.55},
file = {:Users/sebastian/Downloads/01602381.pdf:pdf},
isbn = {0-7695-2536-9},
keywords = {UWA/UWAT+,all or part of,candidate,case study,experience,legacy systems,migration,or hard copies of,permission to make digital,redesign,reengineering,this work for,uwa,uwat,web},
mendeley-tags = {UWA/UWAT+,candidate},
pages = {5 pp.--299},
publisher = {IEEE},
title = {{Towards a holistic approach to redesigning legacy applications for the Web with UWAT+}},
url = {http://dl.acm.org/citation.cfm?id=1134353 http://ieeexplore.ieee.org/document/1602381/},
year = {2006}
}
@article{Liao1998,
abstract = {Case-based reasoning (CBR) is one of the emerging paradigms for designing intelligent systems. Retrieval of similar cases is a primary step in CBR, and the similarity measure plays a very important role in case retrieval. Sometimes CBR systems are called similarity searching systems, the most important characteristic of which is the effectiveness of the similarity measure used to quantify the degree of resemblance between a pair of cases. This article focuses on the similarity measuring methods for CBR and comprises two parts. The first part reviews the existing methods for measuring similarity in the literature based on more than 100 CBR project studies and some general similarity measures seen in other applications. In the second part, a hybrid similarity measure is proposed for comparing cases with a mixture of crisp and fuzzy features. Its application to the domain of failure analysis is illustrated.},
author = {Liao, T. Warren and Zhang, Zhiming and Mount, Claude R.},
doi = {10.1080/088395198117730},
file = {:Users/sebastian/Downloads/Similarity measures for retrieval in case based reasoning systems.pdf:pdf},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
month = {jun},
number = {4},
pages = {267--288},
title = {{Similarity measures for retrieval in case-based reasoning systems}},
url = {http://www.tandfonline.com/doi/abs/10.1080/088395198117730},
volume = {12},
year = {1998}
}
@article{Lucia2007SimilarPages,
abstract = {In this paper, an approach for information systems reverse engineering is proposed and applied. The aim is to support a unified perspective to the reverse engineering process of both data and software. At the state of the art, indeed, many methods, techniques, and tools for software reverse engineering have been proposed to support program comprehension, software maintenance, and software evolution. Other approaches and tools have been proposed for data reverse engineering, with the aim, for example, to provide complete and up-to-date documentation of legacy databases. However, the two engineering communities often worked independently, and very few approaches addressed the reverse engineering of both data and software as information system's constituencies. Hence, a higher integration is needed to support a better co-evolution of databases and programs, in an environment often characterized by high availability of data and volatility of information flows. Accordingly, the approach we propose leverages the detection of object-relational mapping design patterns to build a conceptual schema of the software under analysis. Then, the conceptual schema is mapped to the domain model of the system, to support the design of the evolution of the information system itself. The approach is evaluated on two large-scale open-source enterprise applications. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
archivePrefix = {arXiv},
arxivId = {1408.1293},
author = {Lucia, Andrea De and Scanniello, Giuseppe and Tortora, Genoveffa},
doi = {10.1002/smr.359},
eprint = {1408.1293},
file = {:Users/sebastian/Downloads/Lucia{\_}et{\_}al-2007-Journal{\_}of{\_}Software{\_}Maintenance{\_}and{\_}Evolution{\%}3A{\_}Research{\_}and{\_}Practice.pdf:pdf},
isbn = {9781450330565},
issn = {1532060X},
journal = {Journal of Software Maintenance and Evolution: Research and Practice},
keywords = {Conceptual schema,Design pattern detection,Object-relational mapping,Reverse engineering},
month = {sep},
number = {5},
pages = {281--296},
pmid = {67195556},
title = {{Identifying similar pages in Web applications using a competitive clustering algorithm}},
url = {http://doi.wiley.com/10.1002/smr.359},
volume = {19},
year = {2007}
}
@article{Sneed1995CostBenefit,
abstract = {How can you know if reengineering is cost-effective? If it is$\backslash$npreferable to new development? Or to maintaining the status quo? The$\backslash$nauthor proposes a way to quantify the costs and prove the benefits of$\backslash$nreengineering over other alternatives and offers some advice on$\backslash$ncontracting a reengineering project},
author = {Sneed, Harry M},
doi = {10.1109/52.363168},
file = {:Users/sebastian/Downloads/00363168.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
number = {1},
pages = {24--34},
title = {{Planning the reengineering of legacy systems}},
url = {http://ieeexplore.ieee.org/document/363168/},
volume = {12},
year = {1995}
}
@inproceedings{Oulasvirta2018AIM,
address = {New York, New York, USA},
author = {Oulasvirta, Antti and Miniukovich, Aliaksei and Palmas, Gregorio and Weinkauf, Tino and {De Pascale}, Samuli and Koch, Janin and Langerak, Thomas and Jokinen, Jussi and Todi, Kashyap and Laine, Markku and Kristhombuge, Manoj and Zhu, Yuxi},
booktitle = {The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings - UIST '18 Adjunct},
doi = {10.1145/3266037.3266087},
file = {:Users/sebastian/Downloads/AIM-UIST2018.pdf:pdf},
isbn = {9781450359498},
keywords = {computational evaluation,metrics,ui layouts,user interfaces},
pages = {16--19},
publisher = {ACM Press},
title = {{Aalto Interface Metrics (AIM)): A Service and Codebase for Computational GUI Evaluation}},
url = {http://doi.acm.org/10.1145/3266037.3266087 http://dl.acm.org/citation.cfm?doid=3266037.3266087},
year = {2018}
}
@article{Rubner1998EMD,
author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J},
doi = {10.1023/A:1026543900054},
file = {:Users/sebastian/Downloads/rubner-jcviu-00.pdf:pdf},
journal = {International Journal of Computer Vision},
number = {2},
pages = {99----121},
title = {{The Earch Mover's Distance as a Metric for Image Retrieval}},
url = {http://www.google.co.kr/url?sa=t{\&}rct=j{\&}q=the earth mover's distance as a metric for image retrieval{\&}source=web{\&}cd=1{\&}ved=0CDMQFjAA{\&}url=http{\%}3A{\%}2F{\%}2Fwww.cs.duke.edu{\%}2F{~}tomasi{\%}2Fpapers{\%}2Frubner{\%}2FrubnerTr98.pdf{\&}ei=07ATT5CSMO6tiQf4092PCQ{\&}usg=AFQjCNFH1o86k0hhL},
volume = {40},
year = {1998}
}
@article{Liu2010VIDE,
abstract = {Deep Web contents are accessed by queries submitted to Web databases and the returned data records are enwrapped in dynamically generated Web pages (they will be called deep Web pages in this paper). Extracting structured data from deep Web pages is a challenging problem due to the underlying intricate structures of such pages. Until now, a large number of techniques have been proposed to address this problem, but all of them have inherent limitations because they are Web-page-programming-language-dependent. As the popular two-dimensional media, the contents on Web pages are always displayed regularly for users to browse. This motivates us to seek a different way for deep Web data extraction to overcome the limitations of previous works by utilizing some interesting common visual features on the deep Web pages. In this paper, a novel vision-based approach that is Web-page-programming-language-independent is proposed. This approach primarily utilizes the visual features on the deep Web pages to implement deep Web data extraction, including data record extraction and data item extraction. We also propose a new evaluation measure revision to capture the amount of human effort needed to produce perfect extraction. Our experiments on a large set of Web databases show that the proposed vision-based approach is highly effective for deep Web data extraction.},
author = {Liu, Wei and Meng, Xiaofeng and Meng, Weiyi},
doi = {10.1109/TKDE.2009.109},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Meng, Meng - 2010 - ViDE A Vision-Based Approach for Deep Web Data Extraction.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Visual features of deep Web pages,Web data extraction,Web mining,Wrapper generation},
month = {mar},
number = {3},
pages = {447--460},
title = {{ViDE: A Vision-Based Approach for Deep Web Data Extraction}},
url = {http://ieeexplore.ieee.org/document/4840351/},
volume = {22},
year = {2010}
}
@inproceedings{Semenenko2013Browserbite,
abstract = {Cross-browser compatibility testing is a time consuming and monotonous task. In its most manual form, Web testers open Web pages one-by-one on multiple browser-platform combinations and visually compare the resulting page renderings. Automated cross-browser testing tools speed up this process by extracting screenshots and applying image processing techniques so as to highlight potential incompatibilities. However, these systems suffer from insufficient accuracy, primarily due to a large percentage of false positives. Improving accuracy in this context is challenging as the criteria for classifying a difference as an incompatibility are to some extent subjective. We present our experience building a cross-browser testing tool (Browser bite) based on image segmentation and differencing in conjunction with machine learning. An experimental evaluation involving a dataset of 140 pages, each rendered in 14 browser-system combinations, shows that the use of machine learning in this context leads to significant accuracy improvement, allowing us to attain an F-score of over 90{\%}.},
author = {Semenenko, Nataliia and Dumas, Marlon and Saar, Tonis},
booktitle = {2013 IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2013.88},
file = {:Users/sebastian/Downloads/06676949.pdf:pdf},
isbn = {978-0-7695-4981-1},
keywords = {Cross-browser testing,Image processing,Machine learning},
month = {sep},
pages = {528--531},
publisher = {IEEE},
title = {{Browserbite: Accurate Cross-Browser Testing via Machine Learning over Image Features}},
url = {http://ieeexplore.ieee.org/document/6676949/},
year = {2013}
}
@inproceedings{Talton2011Bricolage,
abstract = {TheWeb provides a corpus of design examples unparalleled in human history. However, leveraging existing designs to produce new pages is often difficult. This paper introduces the Bricolage algorithm for transferring design and content betweenWeb pages. Bricolage employs a novel, structured- prediction technique that learns to create coherent mappings between pages by training on human-generated exemplars. The produced mappings are then used to automatically trans- fer the content from one page into the style and layout of an- other. We show that Bricolage can learn to accurately repro- duce human page mappings, and that it provides a general, efficient, and automatic technique for retargeting content be- tween a variety of realWeb pages.},
address = {New York, New York, USA},
author = {Kumar, Ranjitha and Talton, Jerry O and Ahmad, Salman and Klemmer, Scott R},
booktitle = {Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11},
doi = {10.1145/1978942.1979262},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2011 - Bricolage Example-Based Retargeting for Web Design.pdf:pdf},
isbn = {9781450302289},
pages = {2197},
publisher = {ACM Press},
title = {{Bricolage: Example-Based Retargeting for Web Design}},
url = {http://dl.acm.org/citation.cfm?id=1979262 http://dl.acm.org/citation.cfm?doid=1978942.1979262},
year = {2011}
}
@inproceedings{Bodhuin2002DesktopWebMVC,
abstract = {Integrating legacy Cobol systems into a Web-based architecture is a complex and challenging task. Cobol is not a distributed and object-oriented language, however its integration with other languages or distributed systems is a prerequisite for achieving migration towards Web technologies. Moreover, the user interface and user interaction modalities need to be changed. Numerous strategies have been proposed for wrapping the business logic and re-implementing the user interface, but there is still a great need for experimental research. This paper presents a migration strategy whose target system is a Web-enabled architecture based on the model-view-controller (MVC) design pattern. By extracting all the needed information from the Cobol source code, the realized toolkit can automatically generate wrappers for the business logic and the data model and the Web user interface as Java server pages. The strategy and the toolkit presented have been defined within the project M{\&}S SW, a research project aimed at defining new technological solutions to be transferred to small and medium enterprises operating in information and communication technologies.},
author = {Bodhuin, Thierry and Guardabascio, Enrico and Tortorella, Maria},
booktitle = {Ninth Working Conference on Reverse Engineering, 2002. Proceedings.},
doi = {10.1109/WCRE.2002.1173090},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Bodhuin, Guardabascio, Tortorella - 2002 - Migrating COBOL systems to the Web by using the MVC design pattern.pdf:pdf},
isbn = {0-7695-1799-4},
issn = {10951350},
keywords = {M{\&}S SW,Reverse engineering,candidate},
mendeley-tags = {M{\&}S SW,candidate},
pages = {329--338},
publisher = {IEEE Comput. Soc},
title = {{Migrating COBOL systems to the Web by using the MVC design pattern}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1173090},
year = {2002}
}
@inproceedings{Grechanik2009ICSE,
abstract = {Since manual black-box testing of GUI-based applications (GAPs) is tedious and laborious, test engineers create test scripts to automate the testing process. These test scripts interact with GAPs by performing actions on their GUI objects. An extra effort that test engineers put in writing test scripts is paid off when these scripts are run repeatedly. Unfortunately, releasing new versions of GAPs with modified GUIs breaks their corresponding test scripts thereby obliterating benefits of test automation. We offer a novel approach for maintaining and evolving test scripts so that they can test new versions of their respective GAPs. We built a tool to implement our approach, and we conducted a case study with forty five professional programmers and test engineers to evaluate this tool. The results show with strong statistical significance that users find more failures and report fewer false positives (p {\&}lt; 0.02) in test scripts with our tool than with a flagship industry product and a baseline manual approach. Our tool is lightweight and it takes less than eight seconds to analyze approximately 1KLOC of test scripts.},
author = {Grechanik, Mark and Xie, Qing and Fu, Chen},
booktitle = {2009 IEEE 31st International Conference on Software Engineering},
doi = {10.1109/ICSE.2009.5070540},
file = {:Users/sebastian/Downloads/05070540.pdf:pdf},
isbn = {978-1-4244-3453-4},
issn = {02705257},
keywords = {[Electronic Manuscript]},
pages = {408--418},
publisher = {IEEE},
title = {{Maintaining and evolving GUI-directed test scripts}},
url = {http://ieeexplore.ieee.org/document/5070540/},
year = {2009}
}
@inproceedings{Grechanik2009ICSM,
abstract = {Since manual black-box testing of GUI-based APplications (GAPs) is tedious and laborious, test engineers create test scripts to automate the testing process. These test scripts interact with GAPs by performing actions on their GUI objects. As GAPs evolve, testers should fix their corresponding test scripts so that they can reuse them to test successive releases of GAPs. Currently, there are two main modes of maintaining test scripts: tool-based and manual. In practice, there is no consensus what approach testers should use to maintain test scripts. Test managers make their decisions ad hoc, based on their personal experience and perceived benefits of the tool-based approach versus the manual. In this paper we describe a case study with forty five professional programmers and test engineers to experimentally assess the tool-based approach for maintaining GUIdirected test scripts versus the manual approach. Based on the results of our case study and considering the high cost of the programmers' time and the lower cost of the time of test engineers, and considering that programmers often modify GAP objects in the process of developing software we recommend organizations to supply programmers with testing tools that enable them to fix test scripts faster so that these scripts can unit test software. The other side of our recommendation is that experienced test engineers are likely to be as productive with the manual approach as with the tool-based approach, and we consequently recommend that organizations do not need to provide each tester with an expensive tool license to fix test scripts.},
author = {Grechanik, Mark and Xie, Qing and Fu, Chen},
booktitle = {2009 IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2009.5306345},
file = {:Users/sebastian/Downloads/05306345.pdf:pdf},
isbn = {978-1-4244-4897-5},
month = {sep},
pages = {9--18},
publisher = {IEEE},
title = {{Experimental assessment of manual versus tool-based maintenance of GUI-directed test scripts}},
url = {http://ieeexplore.ieee.org/document/5306345/},
year = {2009}
}
@inproceedings{Grechanik2018,
annote = {extracts GUI trees using accesibility layer from running GUIs
uses tree-edit distance for GUI differencing

experimental study compares to similarity based on normalized linear sums of property differences per all GUI object pairs

experiment data is generated using random GUI tree generation in addition to 3 open source applications},
author = {Grechanik, Mark and Mao, Chi Wu and Baisal, Ankush and Rosenblum, David and Hossain, B.M. Mainul},
booktitle = {2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)},
doi = {10.1109/QRS.2018.00034},
file = {:Users/sebastian/Downloads/08424972.pdf:pdf},
isbn = {978-1-5386-7757-5},
keywords = {Accessibility,Algorithms,Comparison,Differencing,GUI,Generator,Similarity,Testing,Tree-edit},
month = {jul},
pages = {203--214},
publisher = {IEEE},
title = {{Differencing Graphical User Interfaces}},
url = {https://ieeexplore.ieee.org/document/8424972/},
year = {2018}
}
@article{Kang2014,
abstract = {In this work we describe a Convolutional Neural Network (CNN) to accurately predict image quality without a reference image. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max and min pooling, two fully connected layers and an output node. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating image quality. This approach achieves state of the art performance on the LIVE dataset and shows excellent generalization ability in cross dataset experiments. Further experiments on images with local distortions demonstrate the local quality estimation ability of our CNN, which is rarely reported in previous literature.},
author = {Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
doi = {10.1109/CVPR.2014.224},
file = {:Users/sebastian/Downloads/06909620.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Convolutional Neural Network,image quality assessment},
pages = {1733--1740},
title = {{Convolutional neural networks for no-reference image quality assessment}},
year = {2014}
}
@techreport{Lewis2008SMART,
address = {Pittsburgh, PA},
author = {Lewis, Grace and Morris, Edwin and Smith, Dennis and Simanta, Soumya},
file = {:Users/sebastian/Downloads/2008{\_}004{\_}001{\_}14936.pdf:pdf},
institution = {Software Engineering Institute, Carnegie Mellon University},
keywords = {SMART,candidate},
mendeley-tags = {SMART,candidate},
number = {CMU/SEI-2008-TN-008},
title = {{SMART: Analyzing the Reuse Potential of Legacy Components in a Service-Oriented Architecture Environment}},
url = {http://resources.sei.cmu.edu/library/asset-view.cfm?AssetID=8571},
year = {2008}
}
@inproceedings{Miniukovich2015,
abstract = {Live graphical user interfaces (GUIs) do change responding to user actions, unlike GUI screenshots, which are often used in studies. The user experiences and is affected by transitions between the layouts (e.g., webpages or mobile app screens) of interactive systems. Such transitions affect the overall impression of system quality and should be accounted for by any model or computational method estimating the quality and claiming high ecological validity. However, the recent efforts aspiring to predict GUI quality computationally have only relied on homepages or home screens of apps, or their screenshots. The dynamics of GUI – GUI change across pages and layouts, or shorter, visual diversity – have been given little attention. Here we present an initial exploration of GUI visual diversity. In three studies, we demonstrate that a) GUI diversity can be measured computationally; b) GUI diversity correlates with GUI aesthetics impression and other, more high-level GUI-preference constructs; and c) GUI diversity matters in both website and mobile app contexts. We believe the concept of GUI visual diversity deserves further studies.},
address = {New York, New York, USA},
author = {Miniukovich, Aliaksei and {De Angeli}, Antonella},
booktitle = {Proceedings of the 2015 British HCI Conference on - British HCI '15},
doi = {10.1145/2783446.2783580},
file = {:Users/sebastian/Downloads/p101-miniukovich.pdf:pdf},
isbn = {9781450336437},
pages = {101--109},
publisher = {ACM Press},
title = {{Visual diversity and user interface quality}},
url = {http://dl.acm.org/citation.cfm?doid=2783446.2783580},
year = {2015}
}
@inproceedings{Distante2002,
author = {Distante, Damiano and Perrone, V. and Bochicchio, M.A.},
booktitle = {Proceedings. Fourth International Workshop on Web Site Evolution},
doi = {10.1109/WSE.2002.1134095},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Distante, Perrone, Bochicchio - 2002 - Migrating to the Web legacy application the Sinfor project.pdf:pdf},
isbn = {0-7695-1804-4},
keywords = {UWA/UWAT+,Web sites,candidate,reverse engineering},
mendeley-tags = {UWA/UWAT+,candidate},
pages = {85--88},
publisher = {IEEE Comput. Soc},
title = {{Migrating to the Web legacy application: the Sinfor project}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1134095},
year = {2002}
}
@inproceedings{Choudhary2013XPERT,
abstract = {Cross-browser (and cross-platform) issues are prevalent in modern web based applications and range from minor cosmetic bugs to critical functional failures. In spite of the relevance of these issues, cross-browser testing of web applications is still a fairly immature field. Existing tools and techniques require a considerable manual effort to identify such issues and provide limited support to developers for fixing the underlying cause of the issues. To address these limitations, we propose a technique for automatically detecting cross-browser issues and assisting their diagnosis. Our approach is dynamic and is based on differential testing. It compares the behavior of a web application in different web browsers, identifies differences in behavior as potential issues, and reports them to the developers. Given a page to be analyzed, the comparison is performed by combining a structural analysis of the information in the page's DOM and a visual analysis of the page's appearance, obtained through screen captures. To evaluate the usefulness of our approach, we implemented our technique in a tool, called WEBDIFF, and used WEBDIFF to identify cross-browser issues in nine real web applications. The results of our evaluation are promising, in that WEBDIFF was able to automatically identify 121 issues in the applications, while generating only 21 false positives. Moreover, many of these false positives are due to limitations in the current implementation of WEBDIFF and could be eliminated with suitable engineering.},
author = {Choudhary, Shauvik Roy and Prasad, Mukul R. and Orso, Alessandro},
booktitle = {2013 35th International Conference on Software Engineering (ICSE)},
doi = {10.1109/ICSE.2013.6606616},
file = {:Users/sebastian/Downloads/06606616.pdf:pdf},
isbn = {978-1-4673-3076-3},
issn = {02705257},
month = {may},
pages = {702--711},
publisher = {IEEE},
title = {{X-PERT: Accurate identification of cross-browser issues in web applications}},
url = {http://ieeexplore.ieee.org/document/6606616/},
year = {2013}
}
@incollection{VanRooij2016,
abstract = {During the last decade daily life has morphed into a world of broad- band ubiquity, where devices facilitate constant engagement. As a consequence of this, the area of e-commerce has seen an immense growth. Despite the market opportunities for retailers and the ease for customers to acquire products through webshops, the shift to digital retail has its drawbacks. For example, it leads to cluttered and incomparable information among different webshops, which calls for an automated method to regain homogeneity in product representations. This paper presents a product duplicate detection solution, which exploits a data type- driven property alignment framework. Based on the performed experiment, we show a statistically significant improvement of the F1-score from 47.91{\%} to 78.13{\%} compared to an existing state-of-the-art approach.},
annote = {web content similarity analysis},
author = {van Rooij, Gijs and Sewnarain, Ravi and Skogholt, Martin and van der Zaan, Tim and Frasincar, Flavius and Schouten, Kim},
booktitle = {International Conference on Web Information Systems Engineering},
doi = {10.1007/978-3-319-48740-3_28},
file = {:Users/sebastian/Downloads/wise2016a.pdf:pdf},
keywords = {Web products,data types,duplicate detection,property matching,web content similarity analysis},
mendeley-tags = {web content similarity analysis},
pages = {380--395},
title = {{A Data Type-Driven Property Alignment Framework for Product Duplicate Detection on the Web}},
url = {http://link.springer.com/10.1007/978-3-319-48740-3{\_}28},
year = {2016}
}
@article{Saar2016Browserbite,
author = {Saar, T{\~{o}}nis and Dumas, Marlon and Kaljuve, Marti and Semenenko, Nataliia},
doi = {10.1002/spe.2387},
file = {:Users/sebastian/Downloads/Saar{\_}et{\_}al-2016-Software{\_}{\_}Practice{\_}and{\_}Experience.pdf:pdf},
issn = {00380644},
journal = {Software: Practice and Experience},
keywords = {bug finding,linux,program analysis},
month = {nov},
number = {11},
pages = {1459--1477},
title = {{Browserbite: cross-browser testing via image processing}},
url = {http://doi.wiley.com/10.1002/spe.2387},
volume = {46},
year = {2016}
}
@inproceedings{RoyChoudhary2010WebDiff,
abstract = {—Web applications have gained increased popularity in the past decade due to the ubiquity of the web browser across platforms. With the rapid evolution of web technologies, the complexity of web applications has also grown, making maintenance tasks harder. In particular, maintaining cross- browser compliance is a challenging task for web developers, as they must test their application on a variety of browsers and platforms. Existing tools provide some support for this kind of test, but developers are still required to identify and fix cross- browser issues mainly through manual inspection. OurWEBDIFF tool addresses the limitations of existing tools by (1) automatically comparing the structural and visual characteristics of web pages when they are rendered in different browsers, and (2) reporting potential differences to developers. When used on nine real web pages, WEBDIFF automatically identified 121 issues, out of which 100 were actual problems. In this demo, we will present WEBDIFF, its underlying technology, and several examples of its use on real applications.},
author = {{Roy Choudhary}, Shauvik and Versee, Husayn and Orso, Alessandro},
booktitle = {2010 IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2010.5609728},
file = {:Users/sebastian/Downloads/roychoudhary10icsm{\_}tool.pdf:pdf},
isbn = {978-1-4244-8630-4},
keywords = {webdiff},
mendeley-tags = {webdiff},
month = {sep},
pages = {1--6},
publisher = {IEEE},
title = {{A cross-browser web application testing tool}},
url = {http://ieeexplore.ieee.org/document/5609728/},
year = {2010}
}
@inproceedings{Lucia2006,
annote = {Pre-MELIS},
author = {Lucia, Andrea De and Francese, Rita and Scanniello, Giuseppe and Tortora, Genoveffa and Vitiello, Nicola},
booktitle = {2006 22nd IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2006.9},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Lucia et al. - 2006 - A Strategy and an Eclipse Based Environment for the Migration of Legacy Systems to Multi-tier Web-based Architectu.pdf:pdf;:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Lucia et al. - 2006 - A Strategy and an Eclipse Based Environment for the Migration of Legacy Systems to Multi-tier Web-based Archite(2).pdf:pdf},
isbn = {0-7695-2354-4},
issn = {1063-6773},
keywords = {MELIS,candidate},
mendeley-tags = {MELIS,candidate},
month = {sep},
pages = {438--447},
publisher = {IEEE},
title = {{A Strategy and an Eclipse Based Environment for the Migration of Legacy Systems to Multi-tier Web-based Architectures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4021372 http://ieeexplore.ieee.org/document/4021372/},
year = {2006}
}
@incollection{Cajas2019,
annote = {The approach presented by Cajas et al. [@Cajas2019] aims at modernization of legacy web applications to mobile applications. It focuses on adapting the user interface for mobile devices with emphasis on usability. The proposed method is based on process analysis of application features using Markov chains representing screens and the navigation between them. The process analysis is combined with Systematic Layout Planning (SLP) in order to optimise the placement of UI controls on the mobile screen to achieve better alignment with the users' workflow. The analysis requires a group of expert users of different user roles for empirical evaluation to derive the Markov navigation probabilities in the behavior model represented as stationary matrix resulting from multiplication of the initial matrix with a transition matrix. Based on pair-wise comparisons, widgets are then moved, grouped or hidden according to their relationships in the behavior model. The approach does not address phases prior to migration beyond reverse engineering of the behavior model. As WSE approach, both source and target system are web applications. Risk management is not addressed. Since no changes to the backend are proposed, reuse of functionality is addressed, reuse of user interaction is present through the behavior model extraction. The proposed reengineering process requires expertise with Markov chains and is not supported by a tool yet. Integration is not considered, neither on process nor artifacts level.},
author = {Cajas, Viviana and Urbieta, Mat{\'{i}}as and Rybarczyk, Yves and Rossi, Gustavo and Guevara, C{\'{e}}sar},
booktitle = {New Knowledge in Information Systems and Technologies},
doi = {10.1007/978-3-030-16181-1_86},
file = {:Users/sebastian/Downloads/Paper.pdf:pdf},
keywords = {legacy adaptation,markov chains,millennials,mobile devices},
pages = {916--927},
title = {{An Approach for Migrating Legacy Applications to Mobile Interfaces}},
url = {http://link.springer.com/10.1007/978-3-030-16181-1{\_}86},
volume = {3},
year = {2019}
}
@inproceedings{Kumar2013Webzeitgeist,
abstract = {ABSTRACT Advances in data mining and knowledge discovery have transformed the wayWeb sites are designed. However, while visual presentation is an intrinsic part of theWeb, traditional data mining techniques ignore render-time page structures and their attributes. This paper introduces design mining for the Web: using knowledge discovery techniques to under- stand design demographics, automate design curation, and support data-driven design tools. This idea is manifest in Webzeitgeist, a platform for large-scale design mining com- prising a repository of over 100,000Web pages and 100 mil- lion design elements. This paper describes the principles driv- ing design mining, the implementation of the Webzeitgeist architecture, and the new class of data-driven design applica- tions it enables},
address = {New York, New York, USA},
author = {Kumar, Ranjitha and Satyanarayan, Arvind and Torres, Cesar and Lim, Maxine and Ahmad, Salman and Klemmer, Scott R and Talton, Jerry O},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2466420},
isbn = {9781450318990},
pages = {3083},
publisher = {ACM Press},
title = {{Webzeitgeist: Design Mining the Web}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466420},
year = {2013}
}
@inproceedings{RoyChoudhary2014XPERT,
abstract = {Web applications are popular among developers because of their ease of development and deployment through the ubiquitous web browsing platforms. However, differences in a web application's execution across different web browsers can cause cross-browser inconsistencies (XBIs), which are a serious concern for web developers. Identifying XBIs manually is a laborious and error-prone process. In this demo we present X-PERT a tool for identifying XBIs in web applications automatically, without requiring any effort from the developer. X-PERT implements a comprehensive technique for identifying XBIs and has been shown to be effective in detecting real-world XBIs in our empirical evaluation. The source code of X-PERT and XBI reports from our evaluation are available at http://gatech.github.io/xpert. Copyright 2014 ACM.},
address = {New York, New York, USA},
author = {{Roy Choudhary}, Shauvik and Prasad, Mukul R. and Orso, Alessandro},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis - ISSTA 2014},
doi = {10.1145/2610384.2628057},
file = {:Users/sebastian/Downloads/roychoudhary14issta{\_}tool.pdf:pdf},
isbn = {9781450326452},
keywords = {cross-browser testing,layout,web testing},
pages = {417--420},
publisher = {ACM Press},
title = {{X-PERT: a web application testing tool for cross-browser inconsistency detection}},
url = {http://dl.acm.org/citation.cfm?doid=2610384.2628057},
year = {2014}
}
@article{Tuch2009VisualComplexity,
abstract = {Visual complexity is an apparent feature in website design yet its effects on cognitive and emotional processing are not well understood. The current study examined website complexity within the framework of aesthetic theory and psychophysiological research on cognition and emotion. We hypothesized that increasing the complexity of websites would have a detrimental cognitive and emotional impact on users. In a passive viewing task (PVT) 36 website screenshots differing in their degree of complexity (operationalized by JPEG file size; correlation with complexity ratings in a preliminary study r = .80) were presented to 48 participants in randomized order. Additionally, a standardized visual search task (VST) assessing reaction times, and a one-week-delayed recognition task on these websites were conducted and participants rated all websites for arousal and valence. Psychophysiological responses were assessed during the PVT and VST. Visual complexity was related to increased experienced arousal, more negative valence appraisal, decreased heart rate, and increased facial muscle tension (musculus corrugator). Visual complexity resulted in increased reaction times in the VST and decreased recognition rates. Reaction times in the VST were related to increases in heart rate and electrodermal activity. These findings demonstrate that visual complexity of websites has multiple effects on human cognition and emotion, including experienced pleasure and arousal, facial expression, autonomic nervous system activation, task performance, and memory. It should thus be considered an important factor in website design. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Tuch, Alexandre N. and Bargas-Avila, Javier A. and Opwis, Klaus and Wilhelm, Frank H.},
doi = {10.1016/j.ijhcs.2009.04.002},
file = {:Users/sebastian/Downloads/1-s2.0-S107158190900055X-main.pdf:pdf},
issn = {10715819},
journal = {International Journal of Human-Computer Studies},
keywords = {Affect,Affective computing,Attention,First impression,Internet,Psychophysiology,Website aesthetics},
month = {sep},
number = {9},
pages = {703--715},
pmid = {11515286},
publisher = {Elsevier},
title = {{Visual complexity of websites: Effects on users' experience, physiology, performance, and memory}},
url = {http://dx.doi.org/10.1016/j.ijhcs.2009.04.002 http://linkinghub.elsevier.com/retrieve/pii/S107158190900055X},
volume = {67},
year = {2009}
}
@article{Felzenszwalb2010FHOG,
abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin- sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
author = {Felzenszwalb, P F and Girshick, R B and McAllester, D and Ramanan, D},
doi = {10.1109/TPAMI.2009.167},
file = {:Users/sebastian/Downloads/lsvm-pami.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {image detection,machine intelligence,object visualization,patterns},
month = {sep},
number = {9},
pages = {1627--1645},
pmid = {20634557},
title = {{Object Detection with Discriminatively Trained Part-Based Models}},
url = {https://cs.brown.edu/{~}pff/papers/lsvm-pami.pdf http://ieeexplore.ieee.org/document/5255236/},
volume = {32},
year = {2010}
}
@techreport{ARTIST2014Methodology,
abstract = {This document is the D6.2.3 - ARTIST Methodology M30 deliverable report of ARTIST EU Project which summarizes the outcomes of the work performed in the frame of Task 6.2 – ARTIST Methodology definition phase of WP6 - Modernisation Blueprint, methodology and integration in the third year of the project towards the definition of the overall ARTIST Migration and Modernization Methodology.},
author = {Orue-Echeverria, Leire and Alonso, Juncal and Escalante, Marisa and Bruneli{\`{e}}re, Hugo and Canovas, Javier and Strau{\ss}, Oliver and Gorro{\~{n}}ogoitia, Jesus and Pellens, Bram and Konstanteli, Kleopatra and Kousiouris, George and Troya, Javier and Neubauer, Patrick and Wimmer, Manuel},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Orue-Echeverria et al. - 2014 - ARTIST Methodology M30.pdf:pdf},
keywords = {ARTIST,candidate},
mendeley-tags = {ARTIST,candidate},
pages = {15--22},
title = {{ARTIST Methodology M30}},
url = {http://www.artist-project.eu/},
year = {2014}
}
@incollection{Rodriguez-Echeverria2012MIGRARIA,
abstract = {Rodr{\'{i}}guez-Echeverr{\'{i}}a, R., Conejero, J. M., Clemente, P. J., Preciado, J. C., {\&} S{\'{a}}nchez-Figueroa, F. (2011). Modernization of legacy web applications into rich internet applications. In Current Trends in Web Engineering (pp. 236-250). Springer Berlin Heidelberg.},
annote = {focus on MDWE, UI

ADM KDM

static {\&} dynamic analysis {\textgreater} knowledge representation with KDM {\textgreater} (optional) map to RIA-MDWE approach {\textgreater} (optional) refinements {\textgreater} code generation},
author = {Rodr{\'{i}}guez-Echeverr{\'{i}}a, Roberto and Conejero, Jos{\'{e}} Mar{\'{i}}a and Clemente, Pedro J. and Preciado, Juan Carlos and S{\'{a}}nchez-Figueroa, Fernando},
booktitle = {7th Model-Driven Web Engineering Workshop, in conjunction with International Conference on Web Engineering (ICWE)},
doi = {10.1007/978-3-642-27997-3_24},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Rodr{\'{i}}guez-Echeverr{\'{i}}a et al. - 2012 - Modernization of Legacy Web Applications into Rich Internet Applications.pdf:pdf},
keywords = {MIGRARIA,candidate,rich internet applications,service identification,software modernization,software reengineering,web models transformations},
mendeley-tags = {MIGRARIA,candidate,service identification},
pages = {236--250},
title = {{Modernization of Legacy Web Applications into Rich Internet Applications}},
url = {http://link.springer.com/10.1007/978-3-642-27997-3{\_}24},
year = {2012}
}
@article{Liu2017DNNSurvey,
abstract = {Since the proposal of a fast learning algorithm for deep belief networks in 2006, the deep learning techniques have drawn ever-increasing research interests because of their inherent capability of overcoming the drawback of traditional algorithms dependent on hand-designed features. Deep learning approaches have also been found to be suitable for big data analysis with successful applications to computer vision, pattern recognition, speech recognition, natural language processing, and recommendation systems. In this paper, we discuss some widely-used deep learning architectures and their practical applications. An up-to-date overview is provided on four deep learning architectures, namely, autoencoder, convolutional neural network, deep belief network, and restricted Boltzmann machine. Different types of deep neural networks are surveyed and recent progresses are summarized. Applications of deep learning techniques on some selected areas (speech recognition, pattern recognition and computer vision) are highlighted. A list of future research topics are finally given with clear justifications.},
author = {Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E},
doi = {10.1016/j.neucom.2016.12.038},
file = {:Users/sebastian/Downloads/FullText.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autoencoder,Convolutional Neural Network,Deep Belief Network,Deep Learning,Restricted Boltzmann Machine},
month = {apr},
pages = {11--26},
title = {{A survey of deep neural network architectures and their applications}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.12.038 https://linkinghub.elsevier.com/retrieve/pii/S0925231216315533},
volume = {234},
year = {2017}
}
@misc{OMG2012SMM,
author = {{Object Management Group}},
file = {:Users/sebastian/Downloads/formal-18-05-01.pdf:pdf},
keywords = {standard},
mendeley-tags = {standard},
number = {January},
title = {{Structured Metrics Metamodel (SMM)}},
year = {2012}
}
@inproceedings{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, R. and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206848},
file = {:Users/sebastian/Downloads/imagenet{\_}cvpr09.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@inproceedings{Mohagheghi2011REMICS,
abstract = {The key barrier to widespread uptake of cloud computing is the lack of trust in clouds by potential customers. While preventive controls for security and privacy are actively researched, there is still little focus on detective controls related to cloud accountability and auditability. The complexity resulting from large-scale virtualization and data distribution carried out in current clouds has revealed an urgent research agenda for cloud accountability, as has the shift in focus of customer concerns from servers to data. This paper discusses key issues and challenges in achieving a trusted cloud through the use of detective controls, and presents the TrustCloud framework, which addresses accountability in cloud computing via technical and policy-based approaches.},
annote = {complete approach (all ReMiP disciplines)},
author = {Mohagheghi, Parastoo and S{\ae}ther, Thor},
booktitle = {2011 IEEE World Congress on Services},
doi = {10.1109/SERVICES.2011.26},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Mohagheghi, S{\ae}ther - 2011 - Software Engineering Challenges for Migration to the Service Cloud Paradigm Ongoing Work in the REMICS P(2).pdf:pdf},
isbn = {978-1-4577-0879-4},
issn = {2378-3818},
keywords = {Cloud computing,Methodology,Migration,REMICS,Service-oriented architecture,Software engineering,candidate,cloud computing,methodology,migration,service-oriented architecture,software engineering},
mendeley-tags = {REMICS,candidate},
month = {jul},
pages = {507--514},
publisher = {IEEE},
title = {{Software Engineering Challenges for Migration to the Service Cloud Paradigm: Ongoing Work in the REMICS Project}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6012795 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6012736},
year = {2011}
}
@inproceedings{Krasteva2013REMICSAgile,
abstract = {Migration of legacy systems to more advanced technologies and platforms is a current issue for many software organizations. Model-Driven Modernization combined with Software as a Service delivery model is a very promising approach, which possesses a lot of advantages, including reduced costs, automation of migration activities and reuse of system functionality. However, a drawback of such an innovative modernization approach is that it lacks mature software process models to guide its adoption. Thus, a methodology for seamless execution of different migration and deployment activities is quite needed. On the other hand, agile development methods have been successfully adopted in various projects, which partly or thoroughly use the engineering and delivery models exploited in the modernization process. This paper presents how a particular methodology for Model-Driven Modernization with deployment to the Cloud is enriched with agile techniques to address different challenging issues. The extended agile methodology could be used by organizations which have already applied agile software development as well as by organizations that plan to introduce it in their work. Keywords-},
address = {Rome, Italy},
author = {Krasteva, Iva and Stavru, Stavros and Ilieva, Sylvia},
booktitle = {Proceedings of The Eighth International Conference on Internet and Web Applications and Services (ICIW 2013)},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Krasteva, Stavru, Ilieva - 2013 - Agile Model-Driven Modernization to the Service Cloud.pdf:pdf},
isbn = {9781612082806},
keywords = {REMICS,agile methodology,candidate,cloud computing,driven modernization,model-,software as a service},
mendeley-tags = {REMICS,candidate},
pages = {1--9},
publisher = {Xpert Publishing Services},
title = {{Agile Model-Driven Modernization to the Service Cloud}},
year = {2013}
}
@book{Sauro2016,
author = {Sauro, Jeff and Lewis, James R.},
edition = {2},
isbn = {978-0-12-802308-2},
publisher = {Elsevier LTD, Oxford},
title = {{Quantifying the User Experience: Practical Statistics for User Research}},
url = {https://www.sciencedirect.com/book/9780128023082/quantifying-the-user-experience},
year = {2016}
}
@techreport{Cai2003VIPS,
abstract = {A new web content structure analysis based on visual representation is proposed in this paper. Many web applications such as information retrieval, information extraction and automatic page adaptation can benefit from this structure. This paper presents an automatic top-down, tag-tree independent approach to detect web content structure. It simulates how a user understands web layout structure based on his visual perception. Comparing to other existing techniques, our approach is independent to underlying documentation representation such as HTML and works well even when the HTML structure is far different from layout structure. Experiments show satisfactory results.},
author = {Cai, Deng and Yu, Shipeng and Wen, Ji-rong and Ma, Wei-ying},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Cai et al. - Unknown - VIPS A VIsion based Page Segmentation Algorithm.pdf:pdf},
institution = {Microsoft Research},
keywords = {VIPS},
mendeley-tags = {VIPS},
pages = {1--32},
title = {{VIPS : A VIsion based Page Segmentation Algorithm}},
url = {https://www.microsoft.com/en-us/research/publication/vips-a-vision-based-page-segmentation-algorithm/},
year = {2003}
}
@article{Watanabe2018,
abstract = {Web applications can be accessed through a variety of user agent configurations, in which the browser, platform, and device capabilities are not under the control of developers. In order to grant the compatibility of a web application in each environment, developers must manually inspect their web application in a wide variety of devices, platforms, and browsers. Web applications can be rendered inconsistently depending on the browser, the platform, and the device capabilities which are used. Furthermore, the devices' different viewport widths impact the way web applications are rendered in them, in which elements can be resized and change their absolute positions in the display. These adaptation strategies must also be considered in automatic incompatibility detection approaches in the state of the art. Hence, we propose a classification approach for detecting Layout Cross-platform and Cross-browser incompatibilities, which considers the adaptation strategies used in responsive web applications. Our approach is an extension of previous Cross-browser incompatibility detection approaches and has the goal of reducing the cost associated with manual inspections in different devices, platforms, and browsers, by automatically detecting Layout incompatibilities in this scenario. The proposed approach classifies each DOM element which composes a web application as an incompatibility or not, based on its attributes, position, alignment, screenshot, and the viewport width of the browser. We report the results of an experiment conducted with 42 Responsive Web Applications, rendered in three devices (Apple iPhone SE, Apple iPhone 8 Plus, and Motorola Moto G4) and browsers (Google Chrome and Apple Safari). The results (with F-measure of 0.70) showed evidence which quantify the effectiveness of our classification approach, and it could be further enhanced for detecting Cross-platform and Cross-browser incompatibilities. Furthermore, in the experiment, our approach also performed better when compared to a former state-of-the-art classification technique for Cross-browser incompatibilities detection.},
author = {Watanabe, Willian Massami and Am{\^{e}}ndola, Giovana L{\'{a}}zaro and Paes, Fagner Christian},
doi = {10.1145/3316808},
file = {:Users/sebastian/Dropbox/VSR/WIP/TWEB-Review/revised{\_}proot{\_}2.pdf:pdf},
issn = {15591131},
journal = {ACM Transactions on the Web},
keywords = {Cross-browser incompatibilities,Cross-platform incompatibilities,Incompatibilities automatic detection},
month = {mar},
number = {2},
pages = {1--27},
title = {{Layout Cross-Platform and Cross-Browser Incompatibilities Detection using Classification of DOM Elements}},
url = {http://dl.acm.org/citation.cfm?doid=3313948.3316808},
volume = {13},
year = {2019}
}
@techreport{Remics2013RecoverToolkit,
author = {Barbier, Franck and Deltombe, Ga{\"{e}}tan and Rybi{\'{n}}ski, Kamil and Blatkiewicz, S{\l}awomir and {\'{S}}mia{\l}ek, Micha{\l}},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Barbier et al. - 2013 - REMICS Recover Toolkit , Final release.pdf:pdf},
keywords = {REMICS,candidate},
mendeley-tags = {REMICS,candidate},
number = {257793},
pages = {1--18},
title = {{REMICS Recover Toolkit , Final release}},
year = {2013}
}
@book{Kan1996Metrics,
annote = {https://katalog.bibliothek.tu-chemnitz.de/Record/0002309598},
author = {Kan, Stephen H.},
edition = {3. print},
isbn = {0-201-63339-6},
publisher = {Addison Wesley Longman, Inc},
title = {{Metrics and Models in Software Quality Engineering}},
url = {https://dl.acm.org/citation.cfm?id=559784},
year = {1996}
}
@book{Masak2006,
address = {Berlin/Heidelberg},
author = {Masak, Dieter},
doi = {10.1007/3-540-30320-0},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Masak - 2006 - Legacysoftware.pdf:pdf},
isbn = {3-540-25412-9},
publisher = {Springer-Verlag},
series = {Xpert.press},
title = {{Legacysoftware}},
url = {http://link.springer.com/10.1007/3-540-30320-0},
year = {2006}
}
@article{Kong2012,
author = {Kong, Jun and Barkol, Omer and Bergman, Ruth and Pnueli, Ayelet and Schein, Sagi and Zhang, Kang and Zhao, Chunying},
doi = {10.1109/TSMCC.2011.2171335},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Kong et al. - 2012 - Web Interface Interpretation Using Graph Grammars.pdf:pdf},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
keywords = {Data extraction,Grammar,Graphical user interfaces,HTML,HTML source codes,Image segmentation,Internet,SGG,Semantics,Visualization,Web interface interpretation,Web pages,abstract syntax,atomic interface objects,computer-vision,document image processing,document object model structure,graph grammar,graph grammars,image processing,image-processing technology,information retrieval,page segmentation,segmented screen objects,semantic grouping,spatial graph grammar,user interfaces,visual},
mendeley-tags = {computer-vision,visual},
number = {4},
pages = {590--602},
title = {{Web Interface Interpretation Using Graph Grammars}},
volume = {42},
year = {2012}
}
@misc{Wessa2017Statistics,
author = {Wessa, P},
booktitle = {Free Statistics Software (v1.2.1)},
keywords = {online},
mendeley-tags = {online},
title = {{Multiple Regression (v1.0.48)}},
url = {https://www.wessa.net/rwasp{\_}multipleregression.wasp/},
urldate = {2019-03-31},
year = {2017}
}
@inproceedings{Sanoja2014,
abstract = {In this paper we describe Block-o-Matic, a web page$\backslash$nsegmentation framework. It is a hybrid approach inspired by au-$\backslash$ntomated document processing methods and visual-based content$\backslash$nsegmentation techniques. A web page is associated with three$\backslash$nstructures: the DOM tree, the content structure and the logical$\backslash$nstructure. The DOM tree represents the HTML elements of a$\backslash$npage, the content structure organizes page objects according to$\backslash$ncontent's categories and geometry and finally the logical structure$\backslash$nis the result of mapping content structure on the basis of the$\backslash$nhuman-perceptible meaning that conforms the blocks. The logic$\backslash$nstructure represents the final segmentation. The segmentation$\backslash$nprocess is divided into three phases: analysis, understanding and$\backslash$nreconstruction of a web page. An evaluation is proposed in order$\backslash$nto perform the evaluation of web page segmentations based on a$\backslash$nground truth of 400 pages classified into 16 categories. Block-o-$\backslash$nMatic gives promising results.},
author = {Sanoja, Andres and Gancarski, Stephane},
booktitle = {2014 International Conference on Multimedia Computing and Systems (ICMCS)},
doi = {10.1109/ICMCS.2014.6911249},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Sanoja, Gancarski - 2014 - Block-o-Matic A web page segmentation framework.pdf:pdf},
isbn = {978-1-4799-3824-7},
keywords = {correctness,page segmentation,web pages},
month = {apr},
pages = {595--600},
publisher = {IEEE},
title = {{Block-o-Matic: A web page segmentation framework}},
url = {http://ieeexplore.ieee.org/document/6911249/},
volume = {0},
year = {2014}
}
@article{RoyChoudhary2014Matching,
abstract = {...Unfortunately, checking and maintaining the consistency of different versions of an application by hand is not only time consuming, but also error prone. To address this problem, and help developers in this difficult task, we propose an automated technique for matching features across different versions of a multi-platform web application....},
author = {{Roy Choudhary}, Shauvik and Prasad, Mukul R. and Orso, Alessandro},
doi = {10.1145/2610384.2610409},
file = {:Users/sebastian/Downloads/p82-roychoudhary.pdf:pdf},
isbn = {9781450326452},
keywords = {11,8,and even wearable embedded,com,comput-,cross-platform,desktop computers are rapidly,fujitsu,in fact,ing devices,mobile web,mukul,phones and tablets,us},
pages = {82--92},
title = {{Cross-platform feature matching for web applications}},
year = {2014}
}
@inproceedings{Menychtas2013ARTIST,
abstract = {Nowadays Cloud Computing is considered as the ideal environment for engineering, hosting and provisioning applications. A continuously increasing set of cloud-based solutions is available to application owners and developers to tailor their applications exploiting the advanced features of this paradigm for elasticity, high availability and performance. Although these offerings provide many benefits to new applications, they also incorporate constrains to the modernization and migration of legacy applications by obliging the use of specific technologies and explicit architectural design approaches. The modernization and adaptation of legacy applications to cloud environments is a great challenge for all involved stakeholders, not only from the technical perspective, but also in business level with the need to adapt the business processes and models of the modernized application that will be offered from now on, as a service. In this paper we present a novel model-driven approach for the migration of legacy applications in modern cloud environments which covers all aspects and phases of the migration process, as well as an integrated framework that supports all migration process.},
author = {Menychtas, Andreas and Santzaridou, Christina and Kousiouris, George and Varvarigou, Theodora and Orue-Echevarria, Leire and Alonso, Juncal and Gorronogoitia, Jesus and Bruneliere, Hugo and Strauss, Oliver and Senkova, Tatiana and Pellens, Bram and Stuer, Peter},
booktitle = {2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
doi = {10.1109/SYNASC.2013.62},
file = {:Users/sebastian/Downloads/06821179.pdf:pdf},
isbn = {978-1-4799-3036-4},
keywords = {ARTIST,Cloud Computing,Legacy Software,Methodology,Migration,Modelling,Modernization,candidate},
mendeley-tags = {ARTIST,candidate},
month = {sep},
pages = {424--431},
publisher = {IEEE},
title = {{ARTIST Methodology and Framework: A Novel Approach for the Migration of Legacy Software on the Cloud}},
url = {http://ieeexplore.ieee.org/document/6821179/},
year = {2013}
}
@article{Donati2019,
abstract = {The main goal of this work is to understand how to obtain effective transitions when changing the Web user interfaces from one that stimulates negative affective states to one eliciting more positive emotions. The objective is to improve User eXperience (UX) and usability during the interaction. The transitions applied during the user interface adaptation seek to avoid undesired user disorientation, which can be a consequence of the change. A user study with 40 participants tested three types of transition solutions on a Web application: immediate (changes are applied abruptly all together), overview (changes are first previewed through a small window) and gradual (changes are progressively displayed directly on the interface). The overview and gradual transitions have been designed in such a way as to consider the design criteria used to stimulate specific emotions through the initial and final Web user interfaces. We report and discuss the results of the user test, which, amongst other findings, confirmed that users prefer the overview and gradual transitions.},
author = {Donati, Michela and Mori, Giulio and Patern{\`{o}}, Fabio},
doi = {10.1007/s10209-019-00649-y},
issn = {1615-5289},
journal = {Universal Access in the Information Society},
month = {mar},
title = {{Understanding the transitions between web interfaces designed to stimulate specific emotions}},
url = {https://doi.org/10.1007/s10209-019-00649-y http://link.springer.com/10.1007/s10209-019-00649-y},
year = {2019}
}
@book{SWEBOK2014,
address = {Los Alamitos, CA, USA},
author = {{IEEE Computer Society}},
edition = {3rd},
editor = {Bourque, P. and Fairley, R.E.},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Bourque, Fairley - 2014 - Guide to the software engineering body of knowledge (SWEBOK (R)) Version 3.0.pdf:pdf},
isbn = {9780769551661},
publisher = {IEEE Computer Society Press},
title = {{Guide to the software engineering body of knowledge (SWEBOK (R)): Version 3.0}},
year = {2014}
}
@techreport{ISO/IEEE24765Vocabulary,
abstract = {This document provides a common vocabulary applicable to all systems and software engineering work. It was prepared to collect and standardize terminology. This document is intended to serve as a useful reference for those in the information technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute. This document includes references to the active source standards for definitions so that systems and software engineering concepts and requirements can be further explored.},
author = {ISO/IEEE},
booktitle = {ISO/IEC/IEEE 24765:2017(E)},
doi = {10.1109/IEEESTD.2017.8016712},
file = {:Users/sebastian/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2017 - ISOIECIEEE International Standard - Systems and software engineering--Vocabulary.pdf:pdf},
keywords = {24765,IEC Standards,IEC standards,IEEE Computer Society and Project Management Insti,IEEE Standards,IEEE standards,ISO Standards,ISO standards,ISO/IEC/IEEE international standard,Informatino technology,Software engineering,Systems engineering and theoryt,Terminology,computer,dictionary,information technology,project management,software engineering,software engineering concepts,software requirements,standard,systems engineering,terminology standardization,vocabulary},
mendeley-tags = {standard},
month = {aug},
title = {{ISO/IEC/IEEE International Standard - Systems and software engineering--Vocabulary}},
year = {2017}
}
